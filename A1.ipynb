{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text from text_files/file502.txt:\n",
      "Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
      "\n",
      "After lowercase:\n",
      "kit is awesome. i play in my garage just for personal enjoyment not for performances or anything. once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. with the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "after a few weeks of daily use for at least an hour a day it still looks and plays beautifully. overall one of the best purchases i could have made.\n",
      "\n",
      "After tokenization:\n",
      "['kit', 'is', 'awesome', '.', 'i', 'play', 'in', 'my', 'garage', 'just', 'for', 'personal', 'enjoyment', 'not', 'for', 'performances', 'or', 'anything', '.', 'once', 'you', 'take', 'the', 'time', 'to', 'break', 'down', 'all', 'the', 'settings', ',', 'your', 'able', 'to', 'dial', 'in', 'pretty', 'much', 'any', 'kit', 'and', 'sound', '.', 'with', 'the', 'expansion', 'options', 'and', 'the', 'relatively']\n",
      "After removing stopwords:\n",
      "['kit', 'awesome', '.', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', '.', 'take', 'time', 'break', 'settings', ',', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', '.', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', '.', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', '.', 'overall', 'one', 'best', 'purchases', 'could', 'made', '.']\n",
      "After removing punctuation:\n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "After removing blank spaces:\n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file264.txt:\n",
      "I just tested this fog fluid with a 1byone 400W fogger. Two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. This being a hot space I was pleasantly surprised by how long the fog would linger. It would quickly rise to eye level and then just hang there. Another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "Only downside is that the fog is not very dense. \n",
      "\n",
      "After lowercase:\n",
      "i just tested this fog fluid with a 1byone 400w fogger. two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. this being a hot space i was pleasantly surprised by how long the fog would linger. it would quickly rise to eye level and then just hang there. another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "only downside is that the fog is not very dense. \n",
      "\n",
      "After tokenization:\n",
      "['i', 'just', 'tested', 'this', 'fog', 'fluid', 'with', 'a', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'were', 'sufficient', 'to', 'create', 'enough', 'fog', 'layers', 'for', 'a', 'moody', 'atmosphere', 'in', 'a', '2', 'car', 'garage', '.', 'this', 'being', 'a', 'hot', 'space', 'i', 'was', 'pleasantly', 'surprised', 'by', 'how', 'long', 'the', 'fog', 'would', 'linger', '.']\n",
      "After removing stopwords:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', '.', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', '.', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', '.', 'another', 'nice', 'surprise', 'odor-', 'much', ',', 'step', 'middle', 'thick', 'pocket', 'smells', 'like']\n",
      "After removing punctuation:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'odor', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense']\n",
      "After removing blank spaces:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'odor', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file270.txt:\n",
      "Do not let the low price fool you! This is an incredible device with the free mixing software. It has been years since i tracked anything. Back then everything was Solid State and Analogue. With this Scarlett Solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "So if you are new or getting back to it, GET IT!\n",
      "\n",
      "Pro's\n",
      "Customer Support\n",
      "You Tube videos\n",
      "\n",
      "Con's\n",
      "TOO many features with the Scarl\n",
      "\n",
      "After lowercase:\n",
      "do not let the low price fool you! this is an incredible device with the free mixing software. it has been years since i tracked anything. back then everything was solid state and analogue. with this scarlett solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "so if you are new or getting back to it, get it!\n",
      "\n",
      "pro's\n",
      "customer support\n",
      "you tube videos\n",
      "\n",
      "con's\n",
      "too many features with the scarl\n",
      "\n",
      "After tokenization:\n",
      "['do', 'not', 'let', 'the', 'low', 'price', 'fool', 'you', '!', 'this', 'is', 'an', 'incredible', 'device', 'with', 'the', 'free', 'mixing', 'software', '.', 'it', 'has', 'been', 'years', 'since', 'i', 'tracked', 'anything', '.', 'back', 'then', 'everything', 'was', 'solid', 'state', 'and', 'analogue', '.', 'with', 'this', 'scarlett', 'solo', ',', 'a', 'moderate', 'computer', ',', 'and', 'a', 'large']\n",
      "After removing stopwords:\n",
      "['let', 'low', 'price', 'fool', '!', 'incredible', 'device', 'free', 'mixing', 'software', '.', 'years', 'since', 'tracked', 'anything', '.', 'back', 'everything', 'solid', 'state', 'analogue', '.', 'scarlett', 'solo', ',', 'moderate', 'computer', ',', 'large', 'monitor', '(', 'stress', 'large', 'much', 'control', 'software', ')', ',', 'really', 'lay', 'serious', 'tracks', '.', 'new', 'getting', 'back', ',', 'get', '!', \"pro's\"]\n",
      "After removing punctuation:\n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'pros', 'customer', 'support', 'tube', 'videos', 'cons', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable']\n",
      "After removing blank spaces:\n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'pros', 'customer', 'support', 'tube', 'videos', 'cons', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file516.txt:\n",
      "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
      "\n",
      "After lowercase:\n",
      "i'm using several of these on a wall i built. the hangers work great with several different head-stock shapes. i wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone california. i ended up using velcro strips under the hangers that affix over the fretboard. otherwise a great hanger..\n",
      "\n",
      "After tokenization:\n",
      "['i', \"'m\", 'using', 'several', 'of', 'these', 'on', 'a', 'wall', 'i', 'built', '.', 'the', 'hangers', 'work', 'great', 'with', 'several', 'different', 'head-stock', 'shapes', '.', 'i', 'wish', 'there', 'were', 'available', 'bands', ',', 'straps', 'or', 'locking', 'devices', 'to', 'keep', 'instruments', 'secure', 'here', 'in', 'earthquake', 'prone', 'california', '.', 'i', 'ended', 'up', 'using', 'velcro', 'strips', 'under']\n",
      "After removing stopwords:\n",
      "[\"'m\", 'using', 'several', 'wall', 'built', '.', 'hangers', 'work', 'great', 'several', 'different', 'head-stock', 'shapes', '.', 'wish', 'available', 'bands', ',', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', '.', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', '.', 'otherwise', 'great', 'hanger', '..']\n",
      "After removing punctuation:\n",
      "['m', 'using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'headstock', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "After removing blank spaces:\n",
      "['m', 'using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'headstock', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file258.txt:\n",
      "Poor design doesn't line their own pedals up when connected using these. The offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. As you can see in the picture, the third pedal from the chain is already almost off the board. How can they overlook this?\n",
      "\n",
      "After lowercase:\n",
      "poor design doesn't line their own pedals up when connected using these. the offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. as you can see in the picture, the third pedal from the chain is already almost off the board. how can they overlook this?\n",
      "\n",
      "After tokenization:\n",
      "['poor', 'design', 'does', \"n't\", 'line', 'their', 'own', 'pedals', 'up', 'when', 'connected', 'using', 'these', '.', 'the', 'offset', 'is', \"n't\", 'enough', 'and', 'results', 'in', 'each', 'pedal', 'along', 'the', 'line', 'mounted', 'a', 'bit', 'lower', 'than', 'the', 'one', 'next', 'to', 'it', '.', 'as', 'you', 'can', 'see', 'in', 'the', 'picture', ',', 'the', 'third', 'pedal', 'from']\n",
      "After removing stopwords:\n",
      "['poor', 'design', \"n't\", 'line', 'pedals', 'connected', 'using', '.', 'offset', \"n't\", 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', '.', 'see', 'picture', ',', 'third', 'pedal', 'chain', 'already', 'almost', 'board', '.', 'overlook', '?']\n",
      "After removing punctuation:\n",
      "['poor', 'design', 'nt', 'line', 'pedals', 'connected', 'using', 'offset', 'nt', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "After removing blank spaces:\n",
      "['poor', 'design', 'nt', 'line', 'pedals', 'connected', 'using', 'offset', 'nt', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "dataset_dir = 'text_files'\n",
    "preprocessed_dir = 'preprocessed_files'\n",
    "\n",
    "if not os.path.exists(preprocessed_dir):\n",
    "    os.makedirs(preprocessed_dir)\n",
    "\n",
    "def preprocess_files(file_path, save_path, pt5=False):\n",
    "    \n",
    "    # Reading the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    if pt5:\n",
    "        print(f\"Original text from {file_path}:\\n{text[:500]}\\n\")  \n",
    "    \n",
    "    # Lowercase all the text of the file\n",
    "    text_lower = text.lower()\n",
    "    if pt5:\n",
    "        print(f\"After lowercase:\\n{text_lower[:500]}\\n\")  \n",
    "    \n",
    "    # Tokenize the text of the file\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    if pt5:\n",
    "        print(\"After tokenization:\")\n",
    "        print(tokens[:50])  \n",
    "    \n",
    "    # Removeing stopwords from the tokens list\n",
    "    stopwords_set = set(stopwords.words('english')) # importing set of English stopwords from NLTK library\n",
    "    tokens_no_stopwords = [w for w in tokens if w not in stopwords_set] # Filtering out the stopwords from 'tokens' list.\n",
    "    if pt5:\n",
    "        print(\"After removing stopwords:\")\n",
    "        print(tokens_no_stopwords[:50])  \n",
    "    \n",
    "    # Removing punctuation from the tokens list\n",
    "    table = str.maketrans('', '', string.punctuation) # Creating a translation table that maps all punctuation to None (deleting it)\n",
    "    tokens_no_punct = [w.translate(table) for w in tokens_no_stopwords if w.translate(table)] # Using the translation table to remove punctuation from each token\n",
    "    if pt5:\n",
    "        print(\"After removing punctuation:\")\n",
    "        print(tokens_no_punct[:50])  \n",
    "    \n",
    "    # Removing blank space tokens\n",
    "    final_words = [word for word in tokens_no_punct if word.strip()]\n",
    "    if pt5:\n",
    "        print(\"After removing blank spaces:\")\n",
    "        print(final_words[:50])  \n",
    "    \n",
    "    # Saving preprocessed text to a new file\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(final_words))\n",
    "\n",
    "processed_5files = 0\n",
    "# Iterating over each filename in the directory specified by 'dataset_dir'\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    # Constructing full path to the file by joining the directory path with the filename\n",
    "    file_path = os.path.join(dataset_dir, filename)\n",
    "    # Setting the path where the preprocessed file will be saved\n",
    "    preprocessed_path = os.path.join(preprocessed_dir, filename)\n",
    "    # Checking if the current path is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        preprocess_files(file_path, preprocessed_path, pt5=processed_5files < 5)\n",
    "        processed_5files += 1\n",
    "        if processed_5files <= 5:\n",
    "            print(f'-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Unigram Inverted Index and Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "def create_unigram_inverted_index(directory):\n",
    "    inv_index = {} # Initializing an empty dictionary for the inverted index\n",
    "    for filename in os.listdir(directory): # Loop through each file in the directory\n",
    "\n",
    "        file_path = os.path.join(directory, filename) # Creating the full path to the file\n",
    "\n",
    "        if os.path.isfile(file_path): # Checking if the path is a file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file: # Open the file for reading\n",
    "                \n",
    "                for w in file.read().split(): # Iterating each word in the file\n",
    "                    if w in inv_index: # Checking whether the word is already in the index\n",
    "                        if filename not in inv_index[w]:  # Avoiding duplicate file entries\n",
    "                            inv_index[w].append(filename) # Adding the filename to the word's list\n",
    "                    else:\n",
    "                        inv_index[w] = [filename] # Initializing a new list with the filename for new words\n",
    "    return inv_index\n",
    "\n",
    "# Create the inverted index\n",
    "unigram_inverted_index = create_unigram_inverted_index(preprocessed_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the inverted index using pickle\n",
    "def save_inverted_index(index, save_path):\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(index, file)\n",
    "\n",
    "# Save the unigram inverted index to a file\n",
    "save_path = 'unigram_inverted_index.pkl'\n",
    "save_inverted_index(unigram_inverted_index, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the inverted index using pickle\n",
    "def load_inverted_index(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        index = pickle.load(file)\n",
    "    return index\n",
    "\n",
    "# Load the unigram inverted index from a file\n",
    "loaded_unigram_inverted_index = load_inverted_index(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: perfect AND fit AND color\n",
      "Number of documents retrieved for query 1: 1\n",
      "Names of the documents retrieved for query 1: file26.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # a. Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # b. Perform tokenization\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    # c. Remove stopwords\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    tokens_no_stopwords = [w for w in tokens if w not in stopwords_set]\n",
    "    \n",
    "    # d. Remove punctuation from tokens\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens_no_punctuation = [w.translate(table) for w in tokens_no_stopwords]\n",
    "    \n",
    "    # e. Remove blank space tokens\n",
    "    words = [w for w in tokens_no_punctuation if w.strip() != '']\n",
    "    \n",
    "    return words\n",
    "\n",
    "def and_operation(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def or_operation(set1, set2):\n",
    "    return set1.union(set2)\n",
    "\n",
    "def and_not_operation(set1, set2):\n",
    "    return set1 - set2\n",
    "\n",
    "def or_not_operation(set1, set2, all_documents):\n",
    "    return set1.union(all_documents - set2)\n",
    "\n",
    "def execute_query(query_tokens, operations, inverted_index, all_documents):\n",
    "    # Converting query tokens to sets of documents\n",
    "    sets = [set(inverted_index.get(token, [])) for token in query_tokens]\n",
    "    \n",
    "    # Executing operations\n",
    "    result_set = sets[0]\n",
    "    for op, next_set in zip(operations, sets[1:]):\n",
    "        if op == \"AND\":\n",
    "            result_set = and_operation(result_set, next_set)\n",
    "        elif op == \"OR\":\n",
    "            result_set = or_operation(result_set, next_set)\n",
    "        elif op == \"AND NOT\":\n",
    "            result_set = and_not_operation(result_set, next_set)\n",
    "        elif op == \"OR NOT\":\n",
    "            result_set = or_not_operation(result_set, next_set, all_documents)\n",
    "    \n",
    "    return sorted(list(result_set))\n",
    "\n",
    "def reconstruct_query_with_operations(original_query, operations):\n",
    "    # Preprocessing the original query\n",
    "    preprocessed_terms = preprocess_text(original_query)\n",
    "    \n",
    "    # Checkingif the number of operations is not one less than the number of preprocessed terms\n",
    "    if len(operations) != len(preprocessed_terms) - 1:\n",
    "        return \"Issue: The number of operations does not match the number of terms minus one.\"\n",
    "    \n",
    "    # Reconstructing the query by interleaving operations b/w preprocessed terms\n",
    "    reconstructed_query = \"\"\n",
    "    for i, term in enumerate(preprocessed_terms):\n",
    "        if i > 0 and i-1 < len(operations):  # Check to avoid index error\n",
    "            reconstructed_query += f\" {operations[i-1]} \"\n",
    "        reconstructed_query += term\n",
    "    \n",
    "    return reconstructed_query\n",
    "\n",
    "def p_execute_queries(queries, inverted_index, all_documents):\n",
    "    results = []\n",
    "    for i, (original_query, ops) in enumerate(queries, start=1):\n",
    "        query_tokens = preprocess_text(original_query)\n",
    "        operations = ops.split(', ')\n",
    "        # checking if the operations and terms correct in number\n",
    "        if len(query_tokens) - 1 != len(operations):\n",
    "            results.append((i, \"Error you provided incorrect number of operations for the terms\", 0, []))\n",
    "            continue\n",
    "        result_docs = execute_query(query_tokens, operations, inverted_index, all_documents)\n",
    "        reconstructed_query = reconstruct_query_with_operations(original_query, operations)\n",
    "        if \"Issue\" in reconstructed_query:\n",
    "            results.append((i, reconstructed_query, 0, []))\n",
    "            continue\n",
    "        results.append((i, reconstructed_query, len(result_docs), result_docs))\n",
    "    return results\n",
    "\n",
    "N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "queries = []\n",
    "for i in range(N):\n",
    "    input_sequence = input(f\"Enter input sequence for query {i+1}: \")\n",
    "    operations = input(f\"Enter operations (separated by comma) for query {i+1}: \")\n",
    "    queries.append((input_sequence, operations))\n",
    "\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Creating a set of all document names\n",
    "all_documents = set(os.listdir(preprocessed_directory))\n",
    "\n",
    "results = p_execute_queries(queries, loaded_unigram_inverted_index, all_documents)\n",
    "\n",
    "# Printing the results with reconstructed query\n",
    "for i, reconstructed_query, num_docs, docs in results:\n",
    "    print(f\"Query {i}: {reconstructed_query}\")\n",
    "    print(f\"Number of documents retrieved for query {i}: {num_docs}\")\n",
    "    print(f\"Names of the documents retrieved for query {i}: {', '.join(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Positional Index and Phrase Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def default_dict():\n",
    "    return defaultdict(list)\n",
    "\n",
    "def create_positional_index(directory):\n",
    "    positional_index = defaultdict(default_dict)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                words = file.read().split()\n",
    "                for position, word in enumerate(words):\n",
    "                    positional_index[word][filename].append(position)\n",
    "    return positional_index\n",
    "\n",
    "# Saving the positional index\n",
    "def save_positional_index(index, save_path):\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(index, file)\n",
    "\n",
    "# Loading the positional index\n",
    "def load_positional_index(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        index = pickle.load(file)\n",
    "    return index\n",
    "\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "positional_index = create_positional_index(preprocessed_directory)\n",
    "\n",
    "save_path = 'positional_index.pkl'\n",
    "save_positional_index(positional_index, save_path)\n",
    "\n",
    "loaded_positional_index = load_positional_index(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using positional index: 4\n",
      "Names of documents retrieved for query 1 using positional index: file160.txt, file907.txt, file16.txt, file526.txt\n"
     ]
    }
   ],
   "source": [
    "def exe_phrase_query(query_tokens, positional_index):\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "\n",
    "    # Starting with candidate documents for the first token\n",
    "    candidate_docs = positional_index.get(query_tokens[0], {})\n",
    "    \n",
    "    # If single-word query, return its documents directly\n",
    "    if len(query_tokens) == 1:\n",
    "        return list(candidate_docs.keys())\n",
    "\n",
    "    # For phrases, ensuring all the tokens appear in the exact sequence\n",
    "    valid_docs = []\n",
    "    for doc, positions in candidate_docs.items():\n",
    "        for pos in positions:\n",
    "            match = True\n",
    "            for i, token in enumerate(query_tokens[1:], 1):\n",
    "\n",
    "                next_positions = positional_index.get(token, {}).get(doc, [])\n",
    "\n",
    "                if not any(pos + i == next_pos for next_pos in next_positions):\n",
    "                    match = False\n",
    "                    break  # if the position doesn't match the sequence; trying the next position\n",
    "            if match:\n",
    "                valid_docs.append(doc)\n",
    "                break  # Found a matching sequence in this document, no need to check further\n",
    "    return valid_docs\n",
    "\n",
    "def user_input_and_execute_queries(positional_index):\n",
    "    N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "    for i in range(1, N + 1):\n",
    "        \n",
    "        query = input(f\"Enter query {i}: \")\n",
    "        preprocessed_query = preprocess_text(query) \n",
    "        docs_retrieved = exe_phrase_query(preprocessed_query, positional_index)\n",
    "\n",
    "        print(f\"Number of documents retrieved for query {i} using positional index: {len(docs_retrieved)}\")\n",
    "        if docs_retrieved:\n",
    "            print(f\"Names of documents retrieved for query {i} using positional index: {', '.join(docs_retrieved)}\")\n",
    "        else:\n",
    "            print(f\"Names of documents retrieved for query {i} using positional index: None\")\n",
    "\n",
    "user_input_and_execute_queries(loaded_positional_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
