{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text from text_files/file502.txt:\n",
      "Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
      "\n",
      "After lowercase:\n",
      "kit is awesome. i play in my garage just for personal enjoyment not for performances or anything. once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. with the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "after a few weeks of daily use for at least an hour a day it still looks and plays beautifully. overall one of the best purchases i could have made.\n",
      "\n",
      "After tokenization:\n",
      "['kit', 'is', 'awesome', '.', 'i', 'play', 'in', 'my', 'garage', 'just', 'for', 'personal', 'enjoyment', 'not', 'for', 'performances', 'or', 'anything', '.', 'once', 'you', 'take', 'the', 'time', 'to', 'break', 'down', 'all', 'the', 'settings', ',', 'your', 'able', 'to', 'dial', 'in', 'pretty', 'much', 'any', 'kit', 'and', 'sound', '.', 'with', 'the', 'expansion', 'options', 'and', 'the', 'relatively']\n",
      "After removing stopwords:\n",
      "['kit', 'awesome', '.', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', '.', 'take', 'time', 'break', 'settings', ',', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', '.', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', '.', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', '.', 'overall', 'one', 'best', 'purchases', 'could', 'made', '.']\n",
      "After removing punctuation:\n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "After removing blank spaces:\n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file264.txt:\n",
      "I just tested this fog fluid with a 1byone 400W fogger. Two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. This being a hot space I was pleasantly surprised by how long the fog would linger. It would quickly rise to eye level and then just hang there. Another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "Only downside is that the fog is not very dense. \n",
      "\n",
      "After lowercase:\n",
      "i just tested this fog fluid with a 1byone 400w fogger. two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. this being a hot space i was pleasantly surprised by how long the fog would linger. it would quickly rise to eye level and then just hang there. another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "only downside is that the fog is not very dense. \n",
      "\n",
      "After tokenization:\n",
      "['i', 'just', 'tested', 'this', 'fog', 'fluid', 'with', 'a', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'were', 'sufficient', 'to', 'create', 'enough', 'fog', 'layers', 'for', 'a', 'moody', 'atmosphere', 'in', 'a', '2', 'car', 'garage', '.', 'this', 'being', 'a', 'hot', 'space', 'i', 'was', 'pleasantly', 'surprised', 'by', 'how', 'long', 'the', 'fog', 'would', 'linger', '.']\n",
      "After removing stopwords:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', '.', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', '.', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', '.', 'another', 'nice', 'surprise', 'odor-', 'much', ',', 'step', 'middle', 'thick', 'pocket', 'smells', 'like']\n",
      "After removing punctuation:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'odor', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense']\n",
      "After removing blank spaces:\n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'odor', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file270.txt:\n",
      "Do not let the low price fool you! This is an incredible device with the free mixing software. It has been years since i tracked anything. Back then everything was Solid State and Analogue. With this Scarlett Solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "So if you are new or getting back to it, GET IT!\n",
      "\n",
      "Pro's\n",
      "Customer Support\n",
      "You Tube videos\n",
      "\n",
      "Con's\n",
      "TOO many features with the Scarl\n",
      "\n",
      "After lowercase:\n",
      "do not let the low price fool you! this is an incredible device with the free mixing software. it has been years since i tracked anything. back then everything was solid state and analogue. with this scarlett solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "so if you are new or getting back to it, get it!\n",
      "\n",
      "pro's\n",
      "customer support\n",
      "you tube videos\n",
      "\n",
      "con's\n",
      "too many features with the scarl\n",
      "\n",
      "After tokenization:\n",
      "['do', 'not', 'let', 'the', 'low', 'price', 'fool', 'you', '!', 'this', 'is', 'an', 'incredible', 'device', 'with', 'the', 'free', 'mixing', 'software', '.', 'it', 'has', 'been', 'years', 'since', 'i', 'tracked', 'anything', '.', 'back', 'then', 'everything', 'was', 'solid', 'state', 'and', 'analogue', '.', 'with', 'this', 'scarlett', 'solo', ',', 'a', 'moderate', 'computer', ',', 'and', 'a', 'large']\n",
      "After removing stopwords:\n",
      "['let', 'low', 'price', 'fool', '!', 'incredible', 'device', 'free', 'mixing', 'software', '.', 'years', 'since', 'tracked', 'anything', '.', 'back', 'everything', 'solid', 'state', 'analogue', '.', 'scarlett', 'solo', ',', 'moderate', 'computer', ',', 'large', 'monitor', '(', 'stress', 'large', 'much', 'control', 'software', ')', ',', 'really', 'lay', 'serious', 'tracks', '.', 'new', 'getting', 'back', ',', 'get', '!', \"pro's\"]\n",
      "After removing punctuation:\n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'pros', 'customer', 'support', 'tube', 'videos', 'cons', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable']\n",
      "After removing blank spaces:\n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'pros', 'customer', 'support', 'tube', 'videos', 'cons', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file516.txt:\n",
      "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
      "\n",
      "After lowercase:\n",
      "i'm using several of these on a wall i built. the hangers work great with several different head-stock shapes. i wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone california. i ended up using velcro strips under the hangers that affix over the fretboard. otherwise a great hanger..\n",
      "\n",
      "After tokenization:\n",
      "['i', \"'m\", 'using', 'several', 'of', 'these', 'on', 'a', 'wall', 'i', 'built', '.', 'the', 'hangers', 'work', 'great', 'with', 'several', 'different', 'head-stock', 'shapes', '.', 'i', 'wish', 'there', 'were', 'available', 'bands', ',', 'straps', 'or', 'locking', 'devices', 'to', 'keep', 'instruments', 'secure', 'here', 'in', 'earthquake', 'prone', 'california', '.', 'i', 'ended', 'up', 'using', 'velcro', 'strips', 'under']\n",
      "After removing stopwords:\n",
      "[\"'m\", 'using', 'several', 'wall', 'built', '.', 'hangers', 'work', 'great', 'several', 'different', 'head-stock', 'shapes', '.', 'wish', 'available', 'bands', ',', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', '.', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', '.', 'otherwise', 'great', 'hanger', '..']\n",
      "After removing punctuation:\n",
      "['m', 'using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'headstock', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "After removing blank spaces:\n",
      "['m', 'using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'headstock', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file258.txt:\n",
      "Poor design doesn't line their own pedals up when connected using these. The offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. As you can see in the picture, the third pedal from the chain is already almost off the board. How can they overlook this?\n",
      "\n",
      "After lowercase:\n",
      "poor design doesn't line their own pedals up when connected using these. the offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. as you can see in the picture, the third pedal from the chain is already almost off the board. how can they overlook this?\n",
      "\n",
      "After tokenization:\n",
      "['poor', 'design', 'does', \"n't\", 'line', 'their', 'own', 'pedals', 'up', 'when', 'connected', 'using', 'these', '.', 'the', 'offset', 'is', \"n't\", 'enough', 'and', 'results', 'in', 'each', 'pedal', 'along', 'the', 'line', 'mounted', 'a', 'bit', 'lower', 'than', 'the', 'one', 'next', 'to', 'it', '.', 'as', 'you', 'can', 'see', 'in', 'the', 'picture', ',', 'the', 'third', 'pedal', 'from']\n",
      "After removing stopwords:\n",
      "['poor', 'design', \"n't\", 'line', 'pedals', 'connected', 'using', '.', 'offset', \"n't\", 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', '.', 'see', 'picture', ',', 'third', 'pedal', 'chain', 'already', 'almost', 'board', '.', 'overlook', '?']\n",
      "After removing punctuation:\n",
      "['poor', 'design', 'nt', 'line', 'pedals', 'connected', 'using', 'offset', 'nt', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "After removing blank spaces:\n",
      "['poor', 'design', 'nt', 'line', 'pedals', 'connected', 'using', 'offset', 'nt', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure the NLTK datasets are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set the path to your dataset directory and the preprocessed directory\n",
    "dataset_directory = 'text_files'\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Ensure the preprocessed directory exists\n",
    "if not os.path.exists(preprocessed_directory):\n",
    "    os.makedirs(preprocessed_directory)\n",
    "\n",
    "def preprocess_file(file_path, save_path, verbose=False):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original text from {file_path}:\\n{text[:500]}\\n\")  # Print the first 500 characters of the original text\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    if verbose:\n",
    "        print(f\"After lowercase:\\n{text_lower[:500]}\\n\")  # Print the first 500 characters after lowercase\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    if verbose:\n",
    "        print(\"After tokenization:\")\n",
    "        print(tokens[:50])  # Print the first 50 tokens directly\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    tokens_no_stopwords = [w for w in tokens if w not in stopwords_set]\n",
    "    if verbose:\n",
    "        print(\"After removing stopwords:\")\n",
    "        print(tokens_no_stopwords[:50])  # Print the first 50 tokens directly after stopwords removal\n",
    "    \n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens_no_punctuation = [w.translate(table) for w in tokens_no_stopwords if w.translate(table)]\n",
    "    if verbose:\n",
    "        print(\"After removing punctuation:\")\n",
    "        print(tokens_no_punctuation[:50])  # Print the first 50 tokens directly after punctuation removal\n",
    "    \n",
    "    # Remove blank space tokens, if any remain\n",
    "    final_words = [word for word in tokens_no_punctuation if word.strip()]\n",
    "    if verbose:\n",
    "        print(\"After removing blank spaces:\")\n",
    "        print(final_words[:50])  # Print the first 50 tokens directly after removing blank spaces\n",
    "    \n",
    "    # Save the preprocessed text to a new file\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(final_words))\n",
    "\n",
    "files_processed = 0\n",
    "for filename in os.listdir(dataset_directory):\n",
    "    file_path = os.path.join(dataset_directory, filename)\n",
    "    preprocessed_path = os.path.join(preprocessed_directory, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        preprocess_file(file_path, preprocessed_path, verbose=files_processed < 5)\n",
    "        files_processed += 1\n",
    "        if files_processed <= 5:\n",
    "            print(f'-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Unigram Inverted Index and Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assuming the preprocessed files are stored in 'preprocessed_files'\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Function to create a unigram inverted index\n",
    "def create_unigram_inverted_index(directory):\n",
    "    inverted_index = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for word in file.read().split():\n",
    "                    if word in inverted_index:\n",
    "                        if filename not in inverted_index[word]:\n",
    "                            inverted_index[word].append(filename)\n",
    "                    else:\n",
    "                        inverted_index[word] = [filename]\n",
    "    return inverted_index\n",
    "\n",
    "# Create the inverted index\n",
    "unigram_inverted_index = create_unigram_inverted_index(preprocessed_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the inverted index using pickle\n",
    "def save_inverted_index(index, save_path):\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(index, file)\n",
    "\n",
    "# Save the unigram inverted index to a file\n",
    "save_path = 'unigram_inverted_index.pkl'\n",
    "save_inverted_index(unigram_inverted_index, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the inverted index using pickle\n",
    "def load_inverted_index(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        index = pickle.load(file)\n",
    "    return index\n",
    "\n",
    "# Load the unigram inverted index from a file\n",
    "loaded_unigram_inverted_index = load_inverted_index(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: perfect AND fit AND color\n",
      "Number of documents retrieved for query 1: 1\n",
      "Names of the documents retrieved for query 1: file26.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    # Remove punctuation from tokens\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove non-alphabetic tokens, stopwords, and ensure no blank tokens remain\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    words = [w for w in stripped_tokens if w.isalpha() and w not in stopwords_set and w.strip() != '']\n",
    "    \n",
    "    return words\n",
    "\n",
    "def and_operation(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def or_operation(set1, set2):\n",
    "    return set1.union(set2)\n",
    "\n",
    "def and_not_operation(set1, set2):\n",
    "    return set1 - set2\n",
    "\n",
    "def or_not_operation(set1, set2, all_documents):\n",
    "    # Assuming all_documents is a set of all document names\n",
    "    return set1.union(all_documents - set2)\n",
    "\n",
    "def execute_query(query_tokens, operations, inverted_index, all_documents):\n",
    "    # Convert query tokens to sets of documents\n",
    "    sets = [set(inverted_index.get(token, [])) for token in query_tokens]\n",
    "    \n",
    "    # Execute operations\n",
    "    result_set = sets[0]\n",
    "    for op, next_set in zip(operations, sets[1:]):\n",
    "        if op == \"AND\":\n",
    "            result_set = and_operation(result_set, next_set)\n",
    "        elif op == \"OR\":\n",
    "            result_set = or_operation(result_set, next_set)\n",
    "        elif op == \"AND NOT\":\n",
    "            result_set = and_not_operation(result_set, next_set)\n",
    "        elif op == \"OR NOT\":\n",
    "            result_set = or_not_operation(result_set, next_set, all_documents)\n",
    "    \n",
    "    return sorted(list(result_set))\n",
    "\n",
    "def reconstruct_query_with_operations(original_query, operations):\n",
    "    # Preprocess the original query to match the processed terms\n",
    "    preprocessed_terms = preprocess_text(original_query)\n",
    "    \n",
    "    # Reconstruct the query by interleaving operations between preprocessed terms\n",
    "    # Note: This simplistic approach assumes that the number of operations is one less than the number of preprocessed terms\n",
    "    reconstructed_query = \"\"\n",
    "    for i, term in enumerate(preprocessed_terms):\n",
    "        if i > 0 and i-1 < len(operations):  # Check to avoid index error\n",
    "            reconstructed_query += f\" {operations[i-1]} \"\n",
    "        reconstructed_query += term\n",
    "    \n",
    "    return reconstructed_query\n",
    "\n",
    "def parse_and_execute_queries(queries, inverted_index, all_documents):\n",
    "    results = []\n",
    "    for i, (original_query, ops) in enumerate(queries, start=1):\n",
    "        query_tokens = preprocess_text(original_query)\n",
    "        operations = ops.split(', ')\n",
    "        result_docs = execute_query(query_tokens, operations, inverted_index, all_documents)\n",
    "        reconstructed_query = reconstruct_query_with_operations(original_query, operations)\n",
    "        results.append((i, reconstructed_query, len(result_docs), result_docs))\n",
    "    return results\n",
    "\n",
    "# Prompt for the number of queries\n",
    "N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "queries = []\n",
    "for i in range(N):\n",
    "    # For each query, gather the input sequence and the operations\n",
    "    input_sequence = input(f\"Enter input sequence for query {i+1}: \")\n",
    "    operations = input(f\"Enter operations (separated by comma) for query {i+1}: \")\n",
    "    queries.append((input_sequence, operations))\n",
    "\n",
    "# Assuming 'preprocessed_directory' is the path to your directory of preprocessed files\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Create a set of all document names\n",
    "all_documents = set(os.listdir(preprocessed_directory))\n",
    "\n",
    "# Now, you can call 'parse_and_execute_queries' with the correct 'all_documents'\n",
    "results = parse_and_execute_queries(queries, loaded_unigram_inverted_index, all_documents)\n",
    "\n",
    "# Print results with reconstructed query\n",
    "for i, reconstructed_query, num_docs, docs in results:\n",
    "    print(f\"Query {i}: {reconstructed_query}\")\n",
    "    print(f\"Number of documents retrieved for query {i}: {num_docs}\")\n",
    "    print(f\"Names of the documents retrieved for query {i}: {', '.join(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Positional Index and Phrase Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using positional index: 4\n",
      "Names of documents retrieved for query 1 using positional index: file1.txt, file254.txt, file723.txt, file391.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# Redefine the default factory for the positional index\n",
    "def default_factory():\n",
    "    return defaultdict(list)\n",
    "\n",
    "def create_positional_index(directory):\n",
    "    positional_index = defaultdict(default_factory)\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # Assume preprocess_text is defined and properly preprocesses the text\n",
    "                words = preprocess_text(file.read())\n",
    "                for position, word in enumerate(words):\n",
    "                    positional_index[word][filename].append(position)\n",
    "    return positional_index\n",
    "\n",
    "# Save the positional index with pickle\n",
    "def save_positional_index(index, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "\n",
    "# Load the positional index with pickle\n",
    "def load_positional_index(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Execute phrase queries using the positional index\n",
    "def execute_phrase_query(query, positional_index):\n",
    "    words = preprocess_text(query)\n",
    "    if not words:\n",
    "        return []\n",
    "    # Initial list of files containing the first word\n",
    "    potential_files = set(positional_index[words[0]].keys())\n",
    "    for word in words[1:]:\n",
    "        potential_files &= set(positional_index[word].keys())\n",
    "    \n",
    "    valid_docs = []\n",
    "    for file in potential_files:\n",
    "        positions = [positional_index[word][file] for word in words]\n",
    "        for start_pos in zip(*positions):\n",
    "            if all(start_pos[i+1] - start_pos[i] == 1 for i in range(len(start_pos)-1)):\n",
    "                valid_docs.append(file)\n",
    "                break\n",
    "    return valid_docs\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing preprocessed text files\n",
    "    preprocessed_directory = 'preprocessed_files'\n",
    "    index_filename = 'positional_index.pkl'\n",
    "    \n",
    "    # Create and save the positional index\n",
    "    positional_index = create_positional_index(preprocessed_directory)\n",
    "    save_positional_index(positional_index, index_filename)\n",
    "    \n",
    "    # Load the positional index for querying\n",
    "    loaded_positional_index = load_positional_index(index_filename)\n",
    "    \n",
    "    # Execute queries based on user input\n",
    "    N = int(input(\"Enter the number of queries: \"))\n",
    "    for i in range(N):\n",
    "        query = input(f\"Enter phrase query {i+1}: \")\n",
    "        valid_docs = execute_phrase_query(query, loaded_positional_index)\n",
    "        print(f\"Number of documents retrieved for query {i+1} using positional index: {len(valid_docs)}\")\n",
    "        print(f\"Names of documents retrieved for query {i+1} using positional index: {', '.join(valid_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# def default_dict():\n",
    "#     return defaultdict(list)\n",
    "\n",
    "# def create_positional_index(directory):\n",
    "#     positional_index = defaultdict(default_dict)\n",
    "    \n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                 words = file.read().split()\n",
    "#                 for position, word in enumerate(words):\n",
    "#                     positional_index[word][filename].append(position)\n",
    "#     return positional_index\n",
    "\n",
    "# # Saving the positional index\n",
    "# def save_positional_index(index, save_path):\n",
    "#     with open(save_path, 'wb') as file:\n",
    "#         pickle.dump(index, file)\n",
    "\n",
    "# # Loading the positional index\n",
    "# def load_positional_index(load_path):\n",
    "#     with open(load_path, 'rb') as file:\n",
    "#         index = pickle.load(file)\n",
    "#     return index\n",
    "\n",
    "# preprocessed_directory = 'preprocessed_files'\n",
    "# positional_index = create_positional_index(preprocessed_directory)\n",
    "\n",
    "# save_path = 'positional_index.pkl'\n",
    "# save_positional_index(positional_index, save_path)\n",
    "\n",
    "# loaded_positional_index = load_positional_index(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using positional index: 4\n",
      "Names of documents retrieved for query 1 using positional index: file1.txt, file391.txt, file723.txt, file254.txt\n",
      "Number of documents retrieved for query 2 using positional index: 106\n",
      "Names of documents retrieved for query 2 using positional index: file30.txt, file477.txt, file19.txt, file298.txt, file501.txt, file105.txt, file677.txt, file460.txt, file26.txt, file931.txt, file935.txt, file115.txt, file698.txt, file659.txt, file35.txt, file843.txt, file923.txt, file561.txt, file944.txt, file171.txt, file372.txt, file46.txt, file616.txt, file831.txt, file819.txt, file951.txt, file945.txt, file576.txt, file947.txt, file358.txt, file167.txt, file229.txt, file573.txt, file956.txt, file96.txt, file375.txt, file374.txt, file68.txt, file758.txt, file943.txt, file228.txt, file558.txt, file835.txt, file389.txt, file42.txt, file404.txt, file363.txt, file388.txt, file217.txt, file565.txt, file622.txt, file178.txt, file435.txt, file186.txt, file569.txt, file225.txt, file966.txt, file999.txt, file769.txt, file423.txt, file437.txt, file378.txt, file422.txt, file70.txt, file64.txt, file152.txt, file634.txt, file783.txt, file208.txt, file793.txt, file817.txt, file142.txt, file75.txt, file180.txt, file586.txt, file592.txt, file196.txt, file425.txt, file168.txt, file815.txt, file544.txt, file251.txt, file872.txt, file443.txt, file10.txt, file898.txt, file720.txt, file326.txt, file938.txt, file525.txt, file647.txt, file121.txt, file337.txt, file487.txt, file134.txt, file849.txt, file685.txt, file861.txt, file917.txt, file718.txt, file888.txt, file447.txt, file719.txt, file241.txt, file269.txt, file282.txt\n"
     ]
    }
   ],
   "source": [
    "# def execute_phrase_query(query_tokens, positional_index):\n",
    "#     if not query_tokens:\n",
    "#         return []\n",
    "\n",
    "#     # Start with the candidate documents for the first token\n",
    "#     candidate_docs = positional_index.get(query_tokens[0], {})\n",
    "    \n",
    "#     # If it's a single-word query, return its documents directly\n",
    "#     if len(query_tokens) == 1:\n",
    "#         return list(candidate_docs.keys())\n",
    "\n",
    "#     # For phrases, ensure all tokens appear in the exact sequence\n",
    "#     valid_docs = []\n",
    "#     for doc, positions in candidate_docs.items():\n",
    "#         for pos in positions:\n",
    "#             # Assume a match until proven otherwise\n",
    "#             match = True\n",
    "#             for i, token in enumerate(query_tokens[1:], 1):\n",
    "#                 next_positions = positional_index.get(token, {}).get(doc, [])\n",
    "#                 if not any(pos + i == next_pos for next_pos in next_positions):\n",
    "#                     match = False\n",
    "#                     break  # This position doesn't match the sequence; try the next position\n",
    "#             if match:\n",
    "#                 valid_docs.append(doc)\n",
    "#                 break  # Found a matching sequence in this document, no need to check further\n",
    "#     return valid_docs\n",
    "\n",
    "# def user_input_and_execute_queries(positional_index):\n",
    "#     N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "#     for i in range(1, N + 1):\n",
    "#         query = input(f\"Enter query {i}: \")\n",
    "#         preprocessed_query = preprocess_text(query)  # Ensure this function is defined as per your preprocessing steps\n",
    "#         docs_retrieved = execute_phrase_query(preprocessed_query, positional_index)\n",
    "\n",
    "#         print(f\"Number of documents retrieved for query {i} using positional index: {len(docs_retrieved)}\")\n",
    "#         if docs_retrieved:\n",
    "#             print(f\"Names of documents retrieved for query {i} using positional index: {', '.join(docs_retrieved)}\")\n",
    "#         else:\n",
    "#             print(f\"Names of documents retrieved for query {i} using positional index: None\")\n",
    "\n",
    "# # Assuming 'loaded_positional_index' is already defined and loaded\n",
    "# user_input_and_execute_queries(loaded_positional_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
