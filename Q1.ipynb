{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vasanth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text from text_files/file502.txt:\n",
      "Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file264.txt:\n",
      "I just tested this fog fluid with a 1byone 400W fogger. Two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. This being a hot space I was pleasantly surprised by how long the fog would linger. It would quickly rise to eye level and then just hang there. Another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "Only downside is that the fog is not very dense. \n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file270.txt:\n",
      "Do not let the low price fool you! This is an incredible device with the free mixing software. It has been years since i tracked anything. Back then everything was Solid State and Analogue. With this Scarlett Solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "So if you are new or getting back to it, GET IT!\n",
      "\n",
      "Pro's\n",
      "Customer Support\n",
      "You Tube videos\n",
      "\n",
      "Con's\n",
      "TOO many features with the Scarl\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file516.txt:\n",
      "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Original text from text_files/file258.txt:\n",
      "Poor design doesn't line their own pedals up when connected using these. The offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. As you can see in the picture, the third pedal from the chain is already almost off the board. How can they overlook this?\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set the path to your dataset directory\n",
    "dataset_directory = 'text_files'\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Ensure the preprocessed directory exists\n",
    "if not os.path.exists(preprocessed_directory):\n",
    "    os.makedirs(preprocessed_directory)\n",
    "\n",
    "# Function to preprocess a single file\n",
    "def preprocess_file(file_path, save_path, verbose=False):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original text from {file_path}:\\n{text[:500]}\\n\")  # Print the first 500 characters\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    # Remove punctuation from tokens and filter non-alphabetic tokens and stopwords\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    words = [w.translate(table) for w in tokens if w.translate(table).isalpha() and w.translate(table) not in stopwords_set]\n",
    "    \n",
    "    # Remove blank space tokens, if any remain\n",
    "    final_words = [word for word in words if word.strip() != '']\n",
    "    \n",
    "    # Save the preprocessed text to a new file\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(final_words))\n",
    "\n",
    "# Process files in the dataset, printing for the first 5\n",
    "files_processed = 0\n",
    "for filename in os.listdir(dataset_directory):\n",
    "    file_path = os.path.join(dataset_directory, filename)\n",
    "    preprocessed_path = os.path.join(preprocessed_directory, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        preprocess_file(file_path, preprocessed_path, verbose=files_processed < 5)\n",
    "        files_processed += 1\n",
    "        if files_processed <= 5:\n",
    "            print(f'-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Unigram Inverted Index and Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assuming the preprocessed files are stored in 'preprocessed_files'\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Function to create a unigram inverted index\n",
    "def create_unigram_inverted_index(directory):\n",
    "    inverted_index = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for word in file.read().split():\n",
    "                    if word in inverted_index:\n",
    "                        if filename not in inverted_index[word]:\n",
    "                            inverted_index[word].append(filename)\n",
    "                    else:\n",
    "                        inverted_index[word] = [filename]\n",
    "    return inverted_index\n",
    "\n",
    "# Create the inverted index\n",
    "unigram_inverted_index = create_unigram_inverted_index(preprocessed_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the inverted index using pickle\n",
    "def save_inverted_index(index, save_path):\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(index, file)\n",
    "\n",
    "# Save the unigram inverted index to a file\n",
    "save_path = 'unigram_inverted_index.pkl'\n",
    "save_inverted_index(unigram_inverted_index, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the inverted index using pickle\n",
    "def load_inverted_index(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        index = pickle.load(file)\n",
    "    return index\n",
    "\n",
    "# Load the unigram inverted index from a file\n",
    "loaded_unigram_inverted_index = load_inverted_index(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: perfect AND fit AND color\n",
      "Number of documents retrieved for query 1: 1\n",
      "Names of the documents retrieved for query 1: file26.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    # Remove punctuation from tokens\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove non-alphabetic tokens, stopwords, and ensure no blank tokens remain\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    words = [w for w in stripped_tokens if w.isalpha() and w not in stopwords_set and w.strip() != '']\n",
    "    \n",
    "    return words\n",
    "\n",
    "def and_operation(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def or_operation(set1, set2):\n",
    "    return set1.union(set2)\n",
    "\n",
    "def and_not_operation(set1, set2):\n",
    "    return set1 - set2\n",
    "\n",
    "def or_not_operation(set1, set2, all_documents):\n",
    "    # Assuming all_documents is a set of all document names\n",
    "    return set1.union(all_documents - set2)\n",
    "\n",
    "def execute_query(query_tokens, operations, inverted_index, all_documents):\n",
    "    # Convert query tokens to sets of documents\n",
    "    sets = [set(inverted_index.get(token, [])) for token in query_tokens]\n",
    "    \n",
    "    # Execute operations\n",
    "    result_set = sets[0]\n",
    "    for op, next_set in zip(operations, sets[1:]):\n",
    "        if op == \"AND\":\n",
    "            result_set = and_operation(result_set, next_set)\n",
    "        elif op == \"OR\":\n",
    "            result_set = or_operation(result_set, next_set)\n",
    "        elif op == \"AND NOT\":\n",
    "            result_set = and_not_operation(result_set, next_set)\n",
    "        elif op == \"OR NOT\":\n",
    "            result_set = or_not_operation(result_set, next_set, all_documents)\n",
    "    \n",
    "    return sorted(list(result_set))\n",
    "\n",
    "# def parse_and_execute_queries(queries, inverted_index, all_documents):\n",
    "#     results = []\n",
    "#     for i, (query, ops) in enumerate(queries, start=1):\n",
    "#         query_tokens = preprocess_text(query)\n",
    "#         operations = ops.split(', ')\n",
    "#         result_docs = execute_query(query_tokens, operations, inverted_index, all_documents)\n",
    "#         results.append((i, len(result_docs), result_docs))\n",
    "#     return results\n",
    "\n",
    "def reconstruct_query_with_operations(original_query, operations):\n",
    "    # Preprocess the original query to match the processed terms\n",
    "    preprocessed_terms = preprocess_text(original_query)\n",
    "    \n",
    "    # Reconstruct the query by interleaving operations between preprocessed terms\n",
    "    # Note: This simplistic approach assumes that the number of operations is one less than the number of preprocessed terms\n",
    "    reconstructed_query = \"\"\n",
    "    for i, term in enumerate(preprocessed_terms):\n",
    "        if i > 0 and i-1 < len(operations):  # Check to avoid index error\n",
    "            reconstructed_query += f\" {operations[i-1]} \"\n",
    "        reconstructed_query += term\n",
    "    \n",
    "    return reconstructed_query\n",
    "\n",
    "def parse_and_execute_queries(queries, inverted_index, all_documents):\n",
    "    results = []\n",
    "    for i, (original_query, ops) in enumerate(queries, start=1):\n",
    "        query_tokens = preprocess_text(original_query)\n",
    "        operations = ops.split(', ')\n",
    "        result_docs = execute_query(query_tokens, operations, inverted_index, all_documents)\n",
    "        reconstructed_query = reconstruct_query_with_operations(original_query, operations)\n",
    "        results.append((i, reconstructed_query, len(result_docs), result_docs))\n",
    "    return results\n",
    "\n",
    "# Prompt for the number of queries\n",
    "N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "queries = []\n",
    "for i in range(N):\n",
    "    # For each query, gather the input sequence and the operations\n",
    "    input_sequence = input(f\"Enter input sequence for query {i+1}: \")\n",
    "    operations = input(f\"Enter operations (separated by comma) for query {i+1}: \")\n",
    "    queries.append((input_sequence, operations))\n",
    "\n",
    "# Assuming 'preprocessed_directory' is the path to your directory of preprocessed files\n",
    "preprocessed_directory = 'preprocessed_files'\n",
    "\n",
    "# Create a set of all document names\n",
    "all_documents = set(os.listdir(preprocessed_directory))\n",
    "\n",
    "# Now, you can call 'parse_and_execute_queries' with the correct 'all_documents'\n",
    "results = parse_and_execute_queries(queries, loaded_unigram_inverted_index, all_documents)\n",
    "\n",
    "# for i, num_docs, docs in results:\n",
    "#     print(f\"Query {i}:\")\n",
    "#     print(f\"Number of documents retrieved for query {i}: {num_docs}\")\n",
    "#     print(f\"Names of the documents retrieved for query {i}: {', '.join(docs)}\")\n",
    "\n",
    "# Print results with reconstructed query\n",
    "for i, reconstructed_query, num_docs, docs in results:\n",
    "    print(f\"Query {i}: {reconstructed_query}\")\n",
    "    print(f\"Number of documents retrieved for query {i}: {num_docs}\")\n",
    "    print(f\"Names of the documents retrieved for query {i}: {', '.join(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
